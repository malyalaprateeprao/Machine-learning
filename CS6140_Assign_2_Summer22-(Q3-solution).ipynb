{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0259765",
   "metadata": {
    "id": "c0259765"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 2 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775045f",
   "metadata": {
    "id": "e775045f"
   },
   "source": [
    "## Question 1 - Naive Bayes Classification (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9372fb",
   "metadata": {
    "id": "ed9372fb"
   },
   "source": [
    "![Q1_1.png](attachment:Q1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a43e9",
   "metadata": {
    "id": "0a5a43e9"
   },
   "source": [
    "![Q1_2.png](attachment:Q1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98d11d",
   "metadata": {
    "id": "7c98d11d"
   },
   "source": [
    "## Question 2 - Classification Metrics (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb6759",
   "metadata": {
    "id": "1ffb6759"
   },
   "source": [
    "![Q2.png](attachment:Q2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0ce4f",
   "metadata": {
    "id": "0ca0ce4f"
   },
   "source": [
    "## Question 3 -  Logistic Regression and Perceptron  (70 points)\n",
    "\n",
    " In this problem you will be applying logistic regression and perceptron to the breastcancer dataset for binary classification:\n",
    "\n",
    " **default of credit card clients**:  This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbee24d",
   "metadata": {
    "id": "fdbee24d"
   },
   "source": [
    "### Task\n",
    "- Prepare a normalized version of data. Use min-max normalization. \n",
    "- Train two logistic regression models using gradient descent with raw as well as normalized data. \n",
    "- Train two perceptron classifiers with raw as well as normalized data.\n",
    "- Compare training and test results of four models in terms of accuracy. \n",
    "\n",
    "Note:\n",
    "\n",
    "The skeleton code is only a guide. You can change the method definitions where necessary with appropriate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7391b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import librarys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from numpy import log,dot,exp,shape\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f67fca1",
   "metadata": {
    "id": "6f67fca1"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    ''' data: input features\n",
    "        labels: output features\n",
    "    '''\n",
    "    \n",
    "    data = dataset.iloc[:, :-1].values\n",
    "    labels = dataset.iloc[:, -1].values.reshape(-1,1)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2391d",
   "metadata": {
    "id": "2bb2391d"
   },
   "source": [
    "### 1) Implementation of sigmoid and cost function (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f982e9",
   "metadata": {
    "id": "10f982e9"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' return sigmoid'''\n",
    "    \n",
    "    return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0b0962e",
   "metadata": {
    "id": "f0b0962e"
   },
   "outputs": [],
   "source": [
    "## Implement the loss function for logistic regression\n",
    "\n",
    "def compute_cost(y_pred, y_label):\n",
    "    \"\"\"\n",
    "    Cost function in logistic regression where the cost is calculated\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    \n",
    "    len_label=len(y_label)\n",
    "    cost= (-y_label*np.log(y_pred) - (1-y_label)*np.log(1-y_pred)).sum()/len_label\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced13867",
   "metadata": {
    "id": "ced13867"
   },
   "source": [
    "\n",
    "### 2)  Implement logistic regression using batch gradient descent and evaluation (20 points)\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb063fa6",
   "metadata": {
    "id": "cb063fa6"
   },
   "outputs": [],
   "source": [
    "def logistic_regression_using_batch_gradient_descent(x, y, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for logistic regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    cost = np.zeros(max_iter)\n",
    "    mincost = sys.maxsize\n",
    "    mincostweights = None\n",
    "\n",
    "    \n",
    "    theta = np.zeros((shape(x)[1]+1,1))\n",
    "    x = np.c_[np.ones((shape(x)[0],1)),x]\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        \n",
    "        theta = theta - alpha * dot(x.T, sigmoid(dot(x, theta)) - np.reshape(y,(len(y),1)))\n",
    "        y_pred = sigmoid(np.dot(x, theta))\n",
    "        cost[iteration] = compute_cost(y_pred, y)\n",
    "        \n",
    "        if cost[iteration] < mincost:\n",
    "            mincost = cost[iteration]\n",
    "            mincostweights = theta\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "    \n",
    "    return cost, mincostweights\n",
    "\n",
    "def predict(x, weights):\n",
    "    z = dot(np.c_[np.ones((shape(x)[0],1)),x], weights)\n",
    "    lis = []\n",
    "    for i in sigmoid(z):\n",
    "        if i>0.5:\n",
    "            lis.append(1)\n",
    "        else:\n",
    "            lis.append(0)\n",
    "    return lis\n",
    "\n",
    "def F1_score(y,y_hat):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1 and y_hat[i] == 1:\n",
    "            tp += 1\n",
    "        elif y[i] == 1 and y_hat[i] == 0:\n",
    "            fn += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 1:\n",
    "            fp += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 0:\n",
    "            tn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e26ce",
   "metadata": {
    "id": "3f5e26ce"
   },
   "source": [
    "### 3) Implementation of perceptron.(20 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68380bd3",
   "metadata": {
    "id": "68380bd3"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "# constructor \n",
    "    def __init__ (self):\n",
    "        self.w = None   #weights\n",
    "        self.b = None   #bias\n",
    "\n",
    "        \n",
    "    def model(self, x):\n",
    "        return 1 if (np.dot(self.w, x) >= self.b) else 0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            result = self.model(x)\n",
    "            Y.append(result)\n",
    "        return np.array(Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs = 1, lr = 1):\n",
    "    \n",
    "        self.w = np.ones(X.shape[1])\n",
    "        self.b = 0\n",
    "\n",
    "        accuracy = {}\n",
    "        max_accuracy = 0\n",
    "\n",
    "        wt_matrix = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for x, y in zip(X, Y):\n",
    "                y_pred = self.model(x)\n",
    "                if y == 1 and y_pred == 0:\n",
    "                    self.w = self.w + lr * x\n",
    "                    self.b = self.b - lr * 1\n",
    "                elif y == 0 and y_pred == 1:\n",
    "                    self.w = self.w - lr * x\n",
    "                    self.b = self.b + lr * 1\n",
    "\n",
    "            wt_matrix.append(self.w)    \n",
    "            accuracy[i] = accuracy_score(self.predict(X), Y)\n",
    "            if (accuracy[i] > max_accuracy):\n",
    "                max_accuracy = accuracy[i]\n",
    "                j = i\n",
    "                chkptw = self.w\n",
    "                chkptb = self.b\n",
    "\n",
    "        self.w = chkptw\n",
    "        self.b = chkptb\n",
    "\n",
    "        #print(\"Max Training data accuracy =\", max_accuracy, \"iteration\", j)\n",
    "        #print(accuracy.values())\n",
    "        \n",
    "        return np.array(wt_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84b7b4",
   "metadata": {
    "id": "1f84b7b4"
   },
   "source": [
    "### 4) Apply 80-20 split on data to prepare training and test sets. Report training and test results in terms of accuracy, precision and recall for both logistic regression and perceptron. (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cae0d71e",
   "metadata": {
    "id": "cae0d71e"
   },
   "outputs": [],
   "source": [
    "# Sample training code cell change according to your variables and structure\n",
    "\n",
    "# Training the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#reserve the test data, do not use them for cross-validation!\n",
    "\n",
    "dataset = pd.read_excel('Credit card Default.xlsx')\n",
    "data, labels = load_data(dataset)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87fd6a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1916 240000      2 ...      0   3332      0]\n",
      " [  2051  30000      1 ...      0   1500      0]\n",
      " [  2049 120000      2 ...      0      0      0]\n",
      " ...\n",
      " [   800 210000      2 ...   1342   1038   2000]\n",
      " [  4154 280000      2 ...  20669   3003   3250]\n",
      " [  3692 500000      1 ...  24652  18060   3306]]\n",
      "\n",
      "Logistic Regression Raw data details:\n",
      "\n",
      "Raw Training data accuracy 0.7871485943775101\n",
      "Raw Training data precision 0.08068181818181819\n",
      "Raw Training data recall 0.6454545454545455\n",
      "f1 score 0.14343434343434344\n",
      "-----------------\n",
      "Raw Testing data accuracy 0.7941767068273092\n",
      "Raw Testing data precision 0.12217194570135746\n",
      "Raw Testing data recall 0.7105263157894737\n",
      "f1 score 0.2084942084942085\n",
      "\n",
      "Logistic Regression Normalized data details:\n",
      "\n",
      "Normalized Training data accuracy 0.7914156626506024\n",
      "Normalized Training data precision 0.1340909090909091\n",
      "Normalized Training data recall 0.6310160427807486\n",
      "f1 score 0.22118088097469538\n",
      "-----------------\n",
      "Normalized Testing data accuracy 0.7961847389558233\n",
      "Normalized Testing data precision 0.1493212669683258\n",
      "Normalized Testing data recall 0.6875\n",
      "f1 score 0.2453531598513011\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "logistic regression algorithm \n",
    "\n",
    "'''\n",
    "\n",
    "alpha = 0.003     # better learning rates 0.001, 0.003, 0.01, 0.005\n",
    "max_iter = 1000\n",
    "\n",
    "print(x_train)\n",
    "\n",
    "# raw data\n",
    "print(\"\\nLogistic Regression Raw data details:\\n\")\n",
    "\n",
    "\n",
    "standard_x_train = np.copy(x_train)\n",
    "standard_x_test = np.copy(x_test)\n",
    "\n",
    "for i in range(shape(x_train)[1]):\n",
    "    standard_x_train[:,i] = (x_train[:,i] - np.mean(x_train[:,i]))/np.std(x_train[:,i])\n",
    "    \n",
    "for i in range(shape(x_test)[1]):\n",
    "    standard_x_test[:,i] = (x_test[:,i] - np.mean(x_test[:,i]))/np.std(x_test[:,i])\n",
    "\n",
    "cost, mincostweights = logistic_regression_using_batch_gradient_descent(standard_x_train, y_train, alpha, max_iter)\n",
    "\n",
    "Y_pred_train = predict(standard_x_train, mincostweights)\n",
    "print(\"Raw Training data accuracy\", accuracy_score(Y_pred_train, y_train))\n",
    "print(\"Raw Training data precision\", precision_score(Y_pred_train, y_train))\n",
    "print(\"Raw Training data recall\", recall_score(Y_pred_train, y_train))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_train, y_train) * recall_score(Y_pred_train, y_train))/ (precision_score(Y_pred_train, y_train) + recall_score(Y_pred_train, y_train))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "Y_pred_test = predict(standard_x_test, mincostweights)\n",
    "print(\"Raw Testing data accuracy\", accuracy_score(Y_pred_test, y_test))\n",
    "print(\"Raw Testing data precision\", precision_score(Y_pred_test, y_test))\n",
    "print(\"Raw Testing data recall\", recall_score(Y_pred_test, y_test))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_test, y_test, zero_division=1) * recall_score(Y_pred_test, y_test, zero_division=1))/ (precision_score(Y_pred_test, y_test, zero_division=1) + recall_score(Y_pred_test, y_test, zero_division=1))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalized data\n",
    "print(\"\\nLogistic Regression Normalized data details:\\n\")\n",
    "\n",
    "cost, mincostweights = logistic_regression_using_batch_gradient_descent(min_max_scaler.fit_transform(x_train), y_train, alpha, max_iter)\n",
    "\n",
    "Y_pred_train = predict(min_max_scaler.fit_transform(x_train), mincostweights)\n",
    "print(\"Normalized Training data accuracy\", accuracy_score(Y_pred_train, y_train))\n",
    "print(\"Normalized Training data precision\", precision_score(Y_pred_train, y_train))\n",
    "print(\"Normalized Training data recall\", recall_score(Y_pred_train, y_train))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_train, y_train) * recall_score(Y_pred_train, y_train))/ (precision_score(Y_pred_train, y_train) + recall_score(Y_pred_train, y_train))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "Y_pred_test = predict(min_max_scaler.fit_transform(x_test), mincostweights)\n",
    "print(\"Normalized Testing data accuracy\", accuracy_score(Y_pred_test, y_test))\n",
    "print(\"Normalized Testing data precision\", precision_score(Y_pred_test, y_test))\n",
    "print(\"Normalized Testing data recall\", recall_score(Y_pred_test, y_test))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_test, y_test, zero_division=1) * recall_score(Y_pred_test, y_test, zero_division=1))/ (precision_score(Y_pred_test, y_test, zero_division=1) + recall_score(Y_pred_test, y_test, zero_division=1))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474b66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce807648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bbf2dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1916 240000      2 ...      0   3332      0]\n",
      " [  2051  30000      1 ...      0   1500      0]\n",
      " [  2049 120000      2 ...      0      0      0]\n",
      " ...\n",
      " [   800 210000      2 ...   1342   1038   2000]\n",
      " [  4154 280000      2 ...  20669   3003   3250]\n",
      " [  3692 500000      1 ...  24652  18060   3306]]\n",
      "Perceptron raw data details:\n",
      "\n",
      "Raw Training data accuracy 0.7796184738955824\n",
      "Raw Training data precision 0.004545454545454545\n",
      "Raw Training data recall 0.6666666666666666\n",
      "f1 score 0.009029345372460496\n",
      "-------------------\n",
      "Raw Testing data accuracy 0.7781124497991968\n",
      "Raw Testing data precision 0.0\n",
      "Raw Testing data recall 1.0\n",
      "f1 score 0.0\n",
      "\n",
      "Perceptron Normalized data details:\n",
      "\n",
      "Normalized Training data accuracy 0.7886546184738956\n",
      "Normalized Training data precision 0.1590909090909091\n",
      "Normalized Training data recall 0.5785123966942148\n",
      "f1 score 0.24955436720142601\n",
      "-------------------\n",
      "Normalized Testing data accuracy 0.7991967871485943\n",
      "Normalized Testing data precision 0.2895927601809955\n",
      "Normalized Testing data recall 0.5981308411214953\n",
      "f1 score 0.39024390243902446\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "perceptron algorithm \n",
    "\n",
    "'''\n",
    "print(x_train)\n",
    "\n",
    "# For raw data\n",
    "print(\"Perceptron raw data details:\\n\")\n",
    "\n",
    "perceptron = Perceptron()\n",
    "wt_matrix = perceptron.fit(x_train, y_train, 400, 0.0001) # better learning rates 0.0001, 0.01\n",
    "\n",
    "Y_pred_train = perceptron.predict(x_train)\n",
    "print(\"Raw Training data accuracy\", accuracy_score(Y_pred_train, y_train))\n",
    "print(\"Raw Training data precision\", precision_score(Y_pred_train, y_train))\n",
    "print(\"Raw Training data recall\", recall_score(Y_pred_train, y_train))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_train, y_train) * recall_score(Y_pred_train, y_train))/ (precision_score(Y_pred_train, y_train) + recall_score(Y_pred_train, y_train))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "Y_pred_test = perceptron.predict(x_test)\n",
    "print(\"Raw Testing data accuracy\", accuracy_score(Y_pred_test, y_test))\n",
    "print(\"Raw Testing data precision\", precision_score(Y_pred_test, y_test, zero_division=1))\n",
    "print(\"Raw Testing data recall\", recall_score(Y_pred_test, y_test, zero_division=1))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_test, y_test, zero_division=1) * recall_score(Y_pred_test, y_test, zero_division=1))/ (precision_score(Y_pred_test, y_test, zero_division=1) + recall_score(Y_pred_test, y_test, zero_division=1))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For Normalized data\n",
    "print(\"\\nPerceptron Normalized data details:\\n\")\n",
    "\n",
    "perceptron = Perceptron()\n",
    "wt_matrix = perceptron.fit(min_max_scaler.fit_transform(x_train), y_train, 400, 0.01) # better learning rates 0.01, 3, 0.001 \n",
    "\n",
    "\n",
    "Y_pred_train = perceptron.predict(min_max_scaler.fit_transform(x_train))\n",
    "print(\"Normalized Training data accuracy\", accuracy_score(Y_pred_train, y_train))\n",
    "print(\"Normalized Training data precision\", precision_score(Y_pred_train, y_train))\n",
    "print(\"Normalized Training data recall\", recall_score(Y_pred_train, y_train))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_train, y_train) * recall_score(Y_pred_train, y_train))/ (precision_score(Y_pred_train, y_train) + recall_score(Y_pred_train, y_train))\n",
    "print(\"f1 score\", f1_score)\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "Y_pred_test = perceptron.predict(min_max_scaler.fit_transform(x_test))\n",
    "print(\"Normalized Testing data accuracy\", accuracy_score(Y_pred_test, y_test))\n",
    "print(\"Normalized Testing data precision\", precision_score(Y_pred_test, y_test))\n",
    "print(\"Normalized Testing data recall\", recall_score(Y_pred_test, y_test))\n",
    "\n",
    "f1_score = 2 * (precision_score(Y_pred_test, y_test) * recall_score(Y_pred_test, y_test))/ (precision_score(Y_pred_test, y_test) + recall_score(Y_pred_test, y_test))\n",
    "print(\"f1 score\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7792ee2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAcknowledgements:\\n\\n1. for perceptron algorithm i refered this https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6\\n\\n\\n'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "1. for perceptron algorithm i refered this https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6\n",
    "2. for logistic regression algorithm i referred this https://www.analyticsvidhya.com/blog/2022/02/implementing-logistic-regression-from-scratch-using-python/\n",
    "3. I tried to use precision, recall, f1score inbuilt librarays along with that I also used scratch code above for understanding.\n",
    "4. I faced issues with the raw data when doing logistic regression as i was uanble to find cost due to the prediction values are 0. which are\n",
    "   giving nan values as cost, so i used general standardization on raw data.\n",
    "5. I tried with different learning rates () and different sizes of epochs for perceptron and logistic regression.\n",
    "6. Accuracy of normalized data is good compared to raw data for both algorithms.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30c291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS6140-Assign_2_Summer22.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
