{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d6b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import librarys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d7e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  class\n",
       "0       5.0       3.5       1.3       0.3      0\n",
       "1       6.9       3.1       4.9       1.5      1\n",
       "2       5.8       2.6       4.0       1.2      1\n",
       "3       6.7       3.0       5.2       2.3      2\n",
       "4       5.1       3.3       1.7       0.5      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data from the file\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "col_names = data.columns\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c8ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node structure of the tree we creating\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        \n",
    "        #decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        #leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a68c05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        '''constructor of the class'''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split       #minimum samples required in a independent feature for a split\n",
    "        self.max_depth = max_depth       #max depth of the tree\n",
    "        \n",
    "    def build_tree(self, dataset, depth=0):\n",
    "        ''' recursive function for building the decision tree by using training dataset, we also initialized depth of tree as 0 ''' \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]   #separating dataset into X and Y\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        #print('num_samples' , num_samples, 'num_features', num_features)\n",
    "        \n",
    "        # splitting\n",
    "        if num_samples>=self.min_samples_split and depth<=self.max_depth:\n",
    "            \n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            \n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recursively build left sub tree\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], depth+1)\n",
    "                # recursively build right sub tree\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # print(leaf_value)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split, it takes input as training datset, number of samples in dataset and number of features '''\n",
    "        \n",
    "        # dictionary for storing the best split information\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            \n",
    "            feature_values = dataset[:, feature_index] # getting all the values of a feature based on index from dataset\n",
    "            possible_thresholds = np.unique(feature_values) # getting how many unique values are their in that feature values\n",
    "            \n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "                dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "                \n",
    "                # check if dataset_left and datset_right childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    \n",
    "                    # compute information gain using entropy\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    \n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        ''' compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        ''' function to compute entropy '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "    \n",
    "    \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        Y = list(Y)\n",
    "        #print(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        ''' Strating function to train the tree, it takes input as training data and calls build tree for building a tree with that training data '''\n",
    "        \n",
    "        training_dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(training_dataset)\n",
    "        \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)  # if it is a leaf node\n",
    "\n",
    "        else:\n",
    "            print(\"feature_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ''' predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' predict a single data point '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b6e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "#print(X)\n",
    "#print(Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f9dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_2 <= 1.9 ? 0.33741385372714494\n",
      " left:0.0\n",
      " right:feature_2 <= 4.7 ? 0.3836661987599338\n",
      "  left:feature_3 <= 1.6 ? 0.0525931336742147\n",
      "    left:1.0\n",
      "    right:2.0\n",
      "  right:feature_3 <= 1.7 ? 0.054610733182161544\n",
      "    left:feature_0 <= 6.9 ? 0.09999999999999998\n",
      "        left:1.0\n",
      "        right:2.0\n",
      "    right:feature_2 <= 4.8 ? 0.0262345679012347\n",
      "        left:2.0\n",
      "        right:2.0\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\n",
    "classifier.fit(X_train,Y_train)\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f70a24f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15bfda39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nObservations\\n\\n1. If we use entropy as the split feature we are getting accuracy of testing data as 0.9\\n\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Observations\n",
    "\n",
    "1. If we use entropy as the split feature we are getting accuracy of testing data as 0.9\n",
    "2. If we use gini index as the split feature we are getting accuracy of testing data as 0.96, this proves that gini index\n",
    "   performs better than the entropy as the split feature \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aff35b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decision tree from scikit\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "Y_prediction = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ade84ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nObservations\\n\\n1. Here we can see our decision tree and in built decision tree had the same accuracy when we use ginin index as the split feaature in out \\n   decision tree, this tells us that the scikit is also using gini index as the split feature as we see the accuracy is same.\\n\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Observations\n",
    "\n",
    "1. Here we can see our decision tree and in built decision tree had the same accuracy when we use gini index as the split feaature in our \n",
    "   decision tree, this tells us that the scikit is also using gini index as the split feature for building decision tree as we see the accuracy is same.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ccf1058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAcknowledgements:\\n\\n1. I have gone thorugh few online resources for structuring the decision tree in a good way like separating different logics\\n   into different functions, how to use different inbuilt librarys etc\\n2. Followed this video for more understanding and trying to come up with code changes,i used most of the function names and variables same as\\n   https://www.youtube.com/watch?v=sgQAhG5Q7iY\\n3. For building decision tree from scikit I refered geeksforgeeks for knowing the inbuilt librarys\\n\\n\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "1. I have gone thorugh few online resources for structuring the decision tree in a good way like separating different logics\n",
    "   into different functions, how to use different inbuilt librarys etc\n",
    "2. Followed this video for more understanding and trying to come up with code changes,i used most of the function names and variables same as\n",
    "   they are defined in a good way for understanding\n",
    "   https://www.youtube.com/watch?v=sgQAhG5Q7iY\n",
    "3. For building decision tree from scikit I refered geeksforgeeks for knowing the inbuilt librarys.\n",
    "4. As most of the code changes here uses inbuilt librarys from numpy and pandas, so i have learnt how to use this librarys through this assignment\n",
    "5. As i am new to python, i got good exposure to python and its syntaxes, as the logic is common for a decision tree, structring the code in correct format\n",
    "   took lot of time, how to use nodes in python for building a tree, how to specify values for each node etc\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59a1189b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n****************************** end of question1 ************************************\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "****************************** end of question1 ************************************\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0911eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "** learnings**\n",
    "\n",
    "here i added some syntaxes which i built from scratch instead of using python librarys, how to calculate entropy and information gain\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "994f2262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42960493172623937\n",
      "0.21862540660983565\n",
      "0.7110933940492552\n",
      "0.6927156936946566\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Divide dataset into left and right, such that left dataset had all attributes and right dataset had target attribute\n",
    "x = dataset.drop(columns = ['class'])\n",
    "y = dataset['class']\n",
    "\n",
    "#split dataset into training and testing  (70% for training and 30% for testing)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "\n",
    "#print(x_train['feature1'].min())\n",
    "#print(x_train['feature1'].max())\n",
    "\n",
    "gap = x_train['feature1'].max() - x_train['feature1'].min()\n",
    "#print(gap/3)\n",
    "\n",
    "\n",
    "#calculate information gain for each attribute for finding root of decision tree\n",
    "\n",
    "training_dataset_size = len(x_train)\n",
    "testing_dataset_size = len(x_test)\n",
    "\n",
    "t1 = y_train.value_counts()[0]\n",
    "t2 = y_train.value_counts()[1]\n",
    "t3 = y_train.value_counts()[2]\n",
    "\n",
    "entropy_of_target = -((t1/105)*math.log(t1/105,2) + (t2/105)*math.log(t2/105,2) + (t3/105)*math.log(t3/105))\n",
    "#print(entropy_of_target)\n",
    "\n",
    "entropy_of_target = entropy([t1/105, t2/105, t3/105], base=2)\n",
    "#print(entropy_of_target)\n",
    "\n",
    "\n",
    "#decide root node\n",
    "\n",
    "info_gain = []\n",
    "for feature in x.columns:\n",
    "    min = x_train[feature].min()\n",
    "    max = x_train[feature].max()\n",
    "    mid = (min+max)/2\n",
    "    \n",
    "    left = [0,0,0]\n",
    "    right = [0,0,0]\n",
    "    for i,j in zip(x_train[feature], y_train):\n",
    "        if i < mid:\n",
    "            if j == 0:\n",
    "                left[0] = left[0]+1\n",
    "            elif j == 1:\n",
    "                left[1] = left[1]+1\n",
    "            else:\n",
    "                left[2] = left[2]+1\n",
    "        else:\n",
    "            if j == 0:\n",
    "                right[0] = right[0]+1\n",
    "            elif j == 1:\n",
    "                right[1] = right[1]+1\n",
    "            else:\n",
    "                right[2] = right[2]+1\n",
    "    \n",
    "    \n",
    "    total_left = left[0] + left[1] + left[2]        #check >0\n",
    "    total_right = right[0] + right[1] + right[2]\n",
    "    \n",
    "    entropy_left = 0\n",
    "    entropy_right = 0\n",
    "    for i in range(3):\n",
    "        if left[i] > 0:\n",
    "            entropy_left = entropy_left + (left[i]/total_left)*math.log(left[i]/total_left,2);\n",
    "        if right[i] > 0:\n",
    "            entropy_right = entropy_right + (right[i]/total_right)*math.log(right[i]/total_right,2);\n",
    "        \n",
    "        \n",
    "    \n",
    "    #entropy_left = total_left/105 * ((left[0]/total_left)*math.log(left[0]/total_left,2) + (left[1]/total_left)*math.log(left[1]/total_left,2) + (left[2]/total_left)*math.log(left[2]/total_left))\n",
    "    #entropy_right = total_right/105 * ((right[0]/total_right)*math.log(right[0]/total_right,2) + (right[1]/total_right)*math.log(right[1]/total_right,2) + (right[2]/total_right)*math.log(right[2]/total_right))\n",
    "    \n",
    "    entropy_left = -entropy_left * total_left/105;\n",
    "    entropy_right = -entropy_right * total_right/105;\n",
    "    information_gain = entropy_of_target - (entropy_left + entropy_right)\n",
    "    print(information_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237def7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
